//EI SANI Network Specification (prompts):

---
v1a (o3 prompt);

Please implement this specification for a custom pytorch binary neural network;
- The pytorch model config init function is defined as follows; 	
	class EISANIconfig():
		def __init__(self, batchSize, numberOfLayers, hiddenLayerSize, inputLayerSize, outputLayerSize, numberOfFeatures, numberOfClasses, numberOfSynapsesPerSegment):
			self.batchSize = batchSize
			self.numberOfLayers = numberOfLayers
			self.hiddenLayerSize = hiddenLayerSize
			self.inputLayerSize = inputLayerSize
			self.outputLayerSize = outputLayerSize
			self.numberOfFeatures = numberOfFeatures
			self.numberOfClasses = numberOfClasses
			self.numberOfSynapsesPerSegment = numberOfSynapsesPerSegment
			self.numberOfHiddenLayers = numberOfLayers-1
- Initialise the pytorch model;
	- create a tensor of hidden connections for each layer;
		- Create different implementations of hiddenConnectionMatrix depending on the value of bool useSparseMatrix;
			- if useSparseMatrix=False: each neuron (segment) is connected to all previous layer neurons (layer weight matrix of shape=[h_i, h_i-1]), but the weight values are set to 0 if the connections are (currently) off.
			- if useSparseMatrix=True: each neuron (segment) synaptic connections are a set of indices referring to their previous layer neurons (and values corresponding to their weight). hiddenConnectionMatrix is a sparse pytorch tensor.
		- Create different implementations of hiddenConnectionMatrix depending on the value of bool useEIneurons;
			- if useEIneurons=False, create a set of hidden neurons for each hidden layer (hiddenConnectionMatrix). Network weights can either be positive (+1) or negative (-1.) 
			- if useEIneurons=True, create a set of excitatory and inhibitory hidden neurons for each hidden layer (hiddenConnectionMatrixExcitatory and hiddenConnectionMatrixInhibitory). Inhibitory neuron activation function outpus -1 instead of +1. All network weights are positive (+1). There are equal numbers of excitatory and inhibitory connections.
		- Create different implementations of hiddenConnectionMatrix depending on the value of bool useDynamicGeneratedHiddenConnections;
			- if useDynamicGeneratedHiddenConnections=True;
				- A bool tensor called neuronSegmentActivatedMask (shape=[numberOfHiddenLayers, hiddenLayerSize]) is maintained for every neuron in the network and initialised to all False.
				- All connections in network are initialised as off/inactive (0). 
				- During the processing of every train sample, for every new neuron (segment) assigned during train, its connections (of size numberOfSynapsesPerSegment) will be activated (with associated connection weight values).
				- the specific train process is described below ("Create network hidden layer train code").
			- if useDynamicGeneratedHiddenConnections=False; 
				- randomly initialise the connections between each neuron and neurons in the previous layer (numberOfSynapsesPerSegment are assigned between each neuron and its previous layer neurons).
	- create a tensor of output connections for each layer;	
		- For every neuron in every hidden layer in the network a connection is made to every output (class) neuron;
			- A tensor called outputConnectionMatrix (shape=[numberOfHiddenLayers, hiddenLayerSize, numberOfClasses]) is initialised all to 0.
				- If useBinaryOuputConnections=True the outputConnectionMatrix is of type bool, else it is of type float.
- Create forward pass code;
	- The pytorch model forward function is given three arguments; 	
		- bool trainOrTest
		- pytorch tensor input x (shape=[batchSize, numberOfFeatures]), each normalised within range [continuousVarMin:continuousVarMax]
		- optional pytorch tensor target y (shape=[batchSize]), each within the range [0:numberOfClasses]
	- create a tensor of input nodes called inputLayer (shape=[numberOfFeatures*continuousVarEncodingNumBits]).
	- create a tensor of output nodes called outputLayer (shape=[numberOfClasses]).
	- the forward function immediately encodes the continuous variables (x) into binary code (inputLayer);
		- If useGrayCode=True; apply graycode encoding of the input x with continuousVarEncodingNumBits (across range [continuousVarMin:continuousVarMax])
		- If useGrayCode=False; apply thermometer encoding of the input x with continuousVarEncodingNumBits (across range [continuousVarMin:continuousVarMax])
	- implement activation function code (activationFunction);
		- A neuron fires if its total activation is >= segmentActivationThreshold.
	- Create network hidden layer code;	
		- if trainOrTest=True or False, the network propagates the input;
			- For every sample in the batch;
				- For every hidden layer in the network;
					- The pre-activation (Z) values the hidden neurons are calculated in parallel based on its hiddenConnectionMatrix and previous layer neuron activation values.
					- The activation (A) values are calculated in parallel using activationFunction.
		- if trainOrTest=True, the network may train its hidden connections;
			- if useDynamicGeneratedHiddenConnections=True; new hidden connections are created for every new neuron (segment) assigned in the network;
				- For every sample in batch during train, and for every layer in the network;
					- Calculate neuron activations at that hidden layer based on their existing connectivity.
					- If the fraction of activated neurons in the hidden layer is below targetActivationSparsityFraction, then a new neuron (segment) is activated (neuronSegmentActivatedMask[i,j]=True) and its connections are activated;
						- Its connections are assigned based on a random subset of presently activated/nonactivated neurons on the previous layer (numberOfSynapsesPerSegment).
						- If the previous layer neuron being connected is on, then a weight of +1 is assigned to the new neuron (segment) connection.
						- If the previous layer neuron being connected is off, then a weight of -1 is assigned to the new neuron (segment) connection.
					- If there are no more neurons available to be used in the layer (neuronSegmentActivatedMask[h, :] is all True), then a warning is printed but no new connections are assigned.
			- if useDynamicGeneratedHiddenConnections=False; no new hidden connections are created during train (use existing randomly assigned connections). 
	- Create network output layer code;
		- if trainOrTest=True; new output connections are created between active nodes and output (class target) neurons.
			- For every hidden neuron activated, its associated outputConnectionMatrix connection to the current output (class) neuron is updated;
				- If useBinaryOuputConnections=True; the connection weight is incremented (+1.0).
				- If useBinaryOuputConnections=True; the connection weight is set to 1.
		- if trainOrTest=False; the network detects the most activated class neuron.
			- for every hidden neuron activated, its associated output (class) neuron value is updated (by adding its current connection weight value). 
			- the predicted class is selected by selecting the output neuron with the highest activation value.
- Do not hardcode any variables (ie no magic numbers).

---
useDynamicGeneratedHiddenConnectionsUniquenessChecks (o3 prompt);

Please now provide a patch for both _dynamic_hidden_growth and _dynamic_hidden_growth_vectorised that dynamically generates neuron connections only if a neuron does not exist on the current layer with the same connections. The patch needs to work for all cases; is_sparse=True and is_sparse=False, useEIneurons=True and useEIneurons=False. (note if useEIneurons: it verifies an excitatory/inhibitory neuron does not xist on the current layer with the same connections, depending on whether an excitatory or inhibitory neuron is being defined).

---
useImageDataset (o3 prompt);

EISANIpt_EISANImodel.py currently supports tabular datasets (useTabularDataset), where forward argument x is of shape [batchSize, numberOfFeatures]. I want to upgrade EISANIpt_EISANImodel.py to optionally support image datasets (based on the option useImageDataset=True or False).

1. If useImageDataset=True, the forward argument x will be of shape [batchSize, numberOfChannels, inputImageHeight, inputImageWidth] instead of shape [batchSize, numberOfFeatures];
numberOfConvlayers = 6
numberOfLinearLayers = 3
numberOfLayers = numberOfConvlayers+numberOfLinearLayers
numberOfChannels = 3 for the input layer only

2. If useImageDataset=True, execute propagateConvLayers(); a set of convolutional layers (numberOfConvlayers) will be applied before applying standard EISANI linear layers (numberOfLinearLayers);
- Convert each input channel to binary before processing the image, based on a particular threshold EISANICNNinputChannelThreshold;
EISANICNNinputChannelThreshold = 0.5 (assume the input image pixels have been normalised between 0.0 and 1.0 and randomly augmented using torchvision functions). Each channel has a binary value (+1/on or 0/off).
- The model will initialise every possible CNN kernel distribution for each CNN layer, where the CNN kernels have binary weights (+1 or -1) for each "pixel" within their kernel bounds (CNNkernelSize*CNNkernelSize). A threshold (CNNkernelThreshold) is applied to select the final output of a kernel (+1 or 0). The number of channels produced by each CNN layer is therefore calculated by multiplying the previous layer number of channels by 2^(3*3) = 512. Each channel has a binary value (+1/on or 0/off);
	CNNkernelSize = 3
	CNNstride = 1
	CNNkernelThreshold = 5 (ie sum of applied kernel is >= 5)
- If NNmaxPool=True is set, then a standard maxpool operation is performed. The max of each set of 2x2 pixels of the same input channel is calculated after applying each layer CNN (thereby reducing the number of H/W pixels by a factor 2 at each layer).
- If there are 3 conv layers, then the final number of channels before propagating the linear layers will be 3*512*512*512 = 402653184 (approx 400 million), and the number of "pixels" (flattened) will be 32x32/(2x2)/(2x2)/(2x2) = 4x4

3. If useImageDataset=True, the linear layers (numberOfLinearLayers) are initialised, propagated, and updated (if useDynamicGeneratedHiddenConnections=True) using the exact same code as useImageDataset=False (numberOfLayers).

---
numberOfSegmentsPerNeuron (Gemini 2.5 Pro prompt);

Please upgrade the entire EISANI codebase to support multiple segments per neuron (numberOfSegmentsPerNeuron):
- update EISANIpt_EISANI_globalDefs: set numberOfSegmentsPerNeuron = 8.
- update EISANIpt_EISANImodel:_initialise_layer_weights() - 
	- add a new dimension of size numberOfSegmentsPerNeuron in the dense (useSparseMatrix=False) and sparse (useSparseMatrix=True) weight matrices.
	- the dimensions of each weight matrix is now shape=[numNeurons, numberOfSegmentsPerNeuron, prevSize].
- update EISANIpt_EISANImodel:forward():_compute_layer_standard/_compute_layer_EI - when forward propagating the activations, propagate the previous layer input activations into every segment of each hidden layer neuron.
- update EISANIpt_EISANImodel:_neuronActivationFunction() - update the neuron activation function to activate a neuron if any of its segments are active.
- update EISANIpt_EISANImodel:__init__:
	- upgrade neuronSegmentAssignedMask with a new dimension (of size numberOfSegmentsPerNeuron).
	- upgrade hiddenHashes/hiddenHashesExc/hiddenHashesInh with a new dimension (of size numberOfSegmentsPerNeuron).
- update EISANIpt_EISANImodelDynamic:_dynamic_hidden_growth_vectorised()/_dynamic_hidden_growth() -
	- add argument (segmentIndexToUpdate), which defines which segment of the neurons to update. 
	- segmentIndexToUpdate will be defined based on the independent dataset being trained.
	- [the dynamic growth functions will therefore reuse already assigned neurons if their segment index (segmentIndexToUpdate) is not already assigned; otherwise a new neuron will be assigned. This requirement should not need to be explicitly coded, it should naturally emerge from constraining dynamic updates to segmentIndexToUpdate.]
- update EISANIpt_EISANImodelDynamic:perform_uniqueness_check()/perform_uniqueness_check_vectorised() - add argument (segmentIndexToUpdate), which defines which segment of a neuron to perform a uniqueness check.

---
recursiveLayers (Gemini 2.5 Pro prompt);

Please upgrade the entire EISANI codebase to support recursive layers (recursiveLayers):
- update EISANIpt_EISANI_globalDefs: 
	recursiveLayers = True
	if(recursiveLayers): 
		transformerSuperblocks = False
		if(transformerSuperblocks):
			recursiveSuperblocksNumber = 2
		else:
			recursiveSuperblocksNumber = 1
	else:
		recursiveSuperblocksNumber = 1
- update EISANIpt_EISANImodel:EISANImodel:__init__() - 
	if(recursiveLayers): 
		numberUniqueLayers = recursiveSuperblocksNumber*2
		#*2 explanation: the first forward propagated layer in a superblock always uses unique weights:
		#	- for the first superblock this will comprise unique weights between the input layer and the first hidden layer of the superblock
		#	- for the all other superblocks this will comprise unique weights between the previous superblock and the first hidden layer of the superblock
	else:
		numberUniqueLayers = numberOfHiddenLayers
	store unique weights for numberUniqueLayers instead of numberOfHiddenLayers
	store unique outputConnectionMatrix for numberUniqueLayers instead of numberOfHiddenLayers
	store neuronSegmentAssignedMask for numberUniqueLayers instead of numberOfHiddenLayers
	store hiddenHashes* for numberUniqueLayers instead of numberOfHiddenLayers
	etc
- define getUniqueLayerIndex(self, layerIdSuperblock, layerIdHidden):
	if(recursiveLayers):
		if(layerIdHidden==0):
			uniqueLayerIndex = layerIdSuperblock*2
		else:
			uniqueLayerIndex = layerIdSuperblock*2+1
	else:
		uniqueLayerIndex = layerIdHidden
	return uniqueLayerIndex
- update EISANIpt_EISANImodel:forward() - 
	update forward propagation logic;
		for layerIdSuperblock in range(recursiveSuperblocksNumber):
			for layerIdHidden in range(numberOfHiddenLayers):
				uniqueLayerIndex = self.getUniqueLayerIndex(layerIdSuperblock, layerIdHidden)
				pass uniqueLayerIndex to _compute_layer_EI/_compute_layer_standard (layerIdx argument)
				pass uniqueLayerIndex to _dynamic_hidden_growth_vectorised/_dynamic_hidden_growth (layerIdx argument)
	update output connections logic;
		if self.useOutputConnectionsLastLayer;
			...	#same
		else:
			actLayerIndex = 0
			for layerIdSuperblock in range(recursiveSuperblocksNumber):
				for layerIdHidden in range(numberOfHiddenLayers):
					act = layerActivations[actLayerIndex]
					uniqueLayerIndex = self.getUniqueLayerIndex(layerIdSuperblock, layerIdHidden)
					weights = self.outputConnectionMatrix[luniqueLayerIndexayerIdx]
					...	#same
					if trainOrTest and y is not None:
						self._update_output_connections(uniqueLayerIndex, act, y, device)
					actLayerIndex += 1

---
limitOutputConnectionsBasedOnPrevalenceOrExclusivity (o3 prompt);

Please upgrade the implementation of prune_output_connections_based_on_prevalence_and_exclusivity (limitOutputConnectionsBasedOnPrevalenceOrExclusivity=True);
- prune across layers iteratively (starting with the last hidden layer):
	- when pruning output connections from the last hidden layer, use the existing code logic (limitOutputConnectionsBasedOnPrevalence and limitOutputConnectionsBasedOnExclusivity).
	- when pruning output connections from every hidden layer below the last hidden layer, use the existing code logic (limitOutputConnectionsBasedOnPrevalence and limitOutputConnectionsBasedOnExclusivity), but also verify that the hidden neuron does not have any connections remaining to a higher layer hidden neuron.
	- prune hidden neurons without any output connections (that have had all of their output connections pruned); 
		- create bool tensor hiddenNeuronsRemoved to define which hidden neurons must be removed.
		- execute new function pruneHiddenNeurons(layerIndex) to update self.hiddenConnectionMatrix*, self.hiddenNeuronSignatures* and self.neuronSegmentAssignedMask based on neurons recently removed in the layer (hiddenNeuronsRemoved).
