//EI SANI Network Specification (prompts):

---
v1a (o3 prompt);

Please implement this specification for a custom pytorch binary neural network;
- The pytorch model config init function is defined as follows; 	
	class EISANIconfig():
		def __init__(self, batchSize, numberOfLayers, hiddenLayerSize, inputLayerSize, outputLayerSize, numberOfFeatures, numberOfClasses, numberOfSynapsesPerSegment):
			self.batchSize = batchSize
			self.numberOfLayers = numberOfLayers
			self.hiddenLayerSize = hiddenLayerSize
			self.inputLayerSize = inputLayerSize
			self.outputLayerSize = outputLayerSize
			self.numberOfFeatures = numberOfFeatures
			self.numberOfClasses = numberOfClasses
			self.numberOfSynapsesPerSegment = numberOfSynapsesPerSegment
			self.numberOfHiddenLayers = numberOfLayers-1
- Initialise the pytorch model;
	- create a tensor of hidden connections for each layer;
		- Create different implementations of hiddenConnectionMatrix depending on the value of bool useSparseMatrix;
			- if useSparseMatrix=False: each neuron (segment) is connected to all previous layer neurons (layer weight matrix of shape=[h_i, h_i-1]), but the weight values are set to 0 if the connections are (currently) off.
			- if useSparseMatrix=True: each neuron (segment) synaptic connections are a set of indices referring to their previous layer neurons (and values corresponding to their weight). hiddenConnectionMatrix is a sparse pytorch tensor.
		- Create different implementations of hiddenConnectionMatrix depending on the value of bool useEIneurons;
			- if useEIneurons=False, create a set of hidden neurons for each hidden layer (hiddenConnectionMatrix). Network weights can either be positive (+1) or negative (-1.) 
			- if useEIneurons=True, create a set of excitatory and inhibitory hidden neurons for each hidden layer (hiddenConnectionMatrixExcitatory and hiddenConnectionMatrixInhibitory). Inhibitory neuron activation function outpus -1 instead of +1. All network weights are positive (+1). There are equal numbers of excitatory and inhibitory connections.
		- Create different implementations of hiddenConnectionMatrix depending on the value of bool useDynamicGeneratedHiddenConnections;
			- if useDynamicGeneratedHiddenConnections=True;
				- A bool tensor called neuronSegmentActivatedMask (shape=[numberOfHiddenLayers, hiddenLayerSize]) is maintained for every neuron in the network and initialised to all False.
				- All connections in network are initialised as off/inactive (0). 
				- During the processing of every train sample, for every new neuron (segment) assigned during train, its connections (of size numberOfSynapsesPerSegment) will be activated (with associated connection weight values).
				- the specific train process is described below ("Create network hidden layer train code").
			- if useDynamicGeneratedHiddenConnections=False; 
				- randomly initialise the connections between each neuron and neurons in the previous layer (numberOfSynapsesPerSegment are assigned between each neuron and its previous layer neurons).
	- create a tensor of output connections for each layer;	
		- For every neuron in every hidden layer in the network a connection is made to every output (class) neuron;
			- A tensor called outputConnectionMatrix (shape=[numberOfHiddenLayers, hiddenLayerSize, numberOfClasses]) is initialised all to 0.
				- If useBinaryOuputConnections=True the outputConnectionMatrix is of type bool, else it is of type float.
- Create forward pass code;
	- The pytorch model forward function is given three arguments; 	
		- bool trainOrTest
		- pytorch tensor input x (shape=[batchSize, numberOfFeatures]), each normalised within range [continuousVarMin:continuousVarMax]
		- optional pytorch tensor target y (shape=[batchSize]), each within the range [0:numberOfClasses]
	- create a tensor of input nodes called inputLayer (shape=[numberOfFeatures*continuousVarEncodingNumBits]).
	- create a tensor of output nodes called outputLayer (shape=[numberOfClasses]).
	- the forward function immediately encodes the continuous variables (x) into binary code (inputLayer);
		- If useGrayCode=True; apply graycode encoding of the input x with continuousVarEncodingNumBits (across range [continuousVarMin:continuousVarMax])
		- If useGrayCode=False; apply thermometer encoding of the input x with continuousVarEncodingNumBits (across range [continuousVarMin:continuousVarMax])
	- implement activation function code (activationFunction);
		- A neuron fires if its total activation is >= segmentActivationThreshold.
	- Create network hidden layer code;	
		- if trainOrTest=True or False, the network propagates the input;
			- For every sample in the batch;
				- For every hidden layer in the network;
					- The pre-activation (Z) values the hidden neurons are calculated in parallel based on its hiddenConnectionMatrix and previous layer neuron activation values.
					- The activation (A) values are calculated in parallel using activationFunction.
		- if trainOrTest=True, the network may train its hidden connections;
			- if useDynamicGeneratedHiddenConnections=True; new hidden connections are created for every new neuron (segment) assigned in the network;
				- For every sample in batch during train, and for every layer in the network;
					- Calculate neuron activations at that hidden layer based on their existing connectivity.
					- If the fraction of activated neurons in the hidden layer is below targetActivationSparsityFraction, then a new neuron (segment) is activated (neuronSegmentActivatedMask[i,j]=True) and its connections are activated;
						- Its connections are assigned based on a random subset of presently activated/nonactivated neurons on the previous layer (numberOfSynapsesPerSegment).
						- If the previous layer neuron being connected is on, then a weight of +1 is assigned to the new neuron (segment) connection.
						- If the previous layer neuron being connected is off, then a weight of -1 is assigned to the new neuron (segment) connection.
					- If there are no more neurons available to be used in the layer (neuronSegmentActivatedMask[h, :] is all True), then a warning is printed but no new connections are assigned.
			- if useDynamicGeneratedHiddenConnections=False; no new hidden connections are created during train (use existing randomly assigned connections). 
	- Create network output layer code;
		- if trainOrTest=True; new output connections are created between active nodes and output (class target) neurons.
			- For every hidden neuron activated, its associated outputConnectionMatrix connection to the current output (class) neuron is updated;
				- If useBinaryOuputConnections=True; the connection weight is incremented (+1.0).
				- If useBinaryOuputConnections=True; the connection weight is set to 1.
		- if trainOrTest=False; the network detects the most activated class neuron.
			- for every hidden neuron activated, its associated output (class) neuron value is updated (by adding its current connection weight value). 
			- the predicted class is selected by selecting the output neuron with the highest activation value.
- Do not hardcode any variables (ie no magic numbers).

---
useDynamicGeneratedHiddenConnectionsUniquenessChecks (o3 prompt);

Please now provide a patch for both _dynamic_hidden_growth and _dynamic_hidden_growth_vectorised that dynamically generates neuron connections only if a neuron does not exist on the current layer with the same connections. The patch needs to work for all cases; is_sparse=True and is_sparse=False, useEIneurons=True and useEIneurons=False. (note if useEIneurons: it verifies an excitatory/inhibitory neuron does not xist on the current layer with the same connections, depending on whether an excitatory or inhibitory neuron is being defined).

---
useImageDataset (o3 prompt);

EISANIpt_EISANImodel.py currently supports tabular datasets (useTabularDataset), where forward argument x is of shape [batchSize, numberOfFeatures]. I want to upgrade EISANIpt_EISANImodel.py to optionally support image datasets (based on the option useImageDataset=True or False).

1. If useImageDataset=True, the forward argument x will be of shape [batchSize, numberOfChannels, inputImageHeight, inputImageWidth] instead of shape [batchSize, numberOfFeatures];
numberOfConvlayers = 6
numberOfLinearLayers = 3
numberOfLayers = numberOfConvlayers+numberOfLinearLayers
numberOfChannels = 3 for the input layer only

2. If useImageDataset=True, execute propagateConvLayers(); a set of convolutional layers (numberOfConvlayers) will be applied before applying standard EISANI linear layers (numberOfLinearLayers);
- Convert each input channel to binary before processing the image, based on a particular threshold EISANICNNinputChannelThreshold;
EISANICNNinputChannelThreshold = 0.5 (assume the input image pixels have been normalised between 0.0 and 1.0 and randomly augmented using torchvision functions). Each channel has a binary value (+1/on or 0/off).
- The model will initialise every possible CNN kernel distribution for each CNN layer, where the CNN kernels have binary weights (+1 or -1) for each "pixel" within their kernel bounds (CNNkernelSize*CNNkernelSize). A threshold (CNNkernelThreshold) is applied to select the final output of a kernel (+1 or 0). The number of channels produced by each CNN layer is therefore calculated by multiplying the previous layer number of channels by 2^(3*3) = 512. Each channel has a binary value (+1/on or 0/off);
	CNNkernelSize = 3
	CNNstride = 1
	CNNkernelThreshold = 5 (ie sum of applied kernel is >= 5)
- If NNmaxPool=True is set, then a standard maxpool operation is performed. The max of each set of 2x2 pixels of the same input channel is calculated after applying each layer CNN (thereby reducing the number of H/W pixels by a factor 2 at each layer).
- If there are 3 conv layers, then the final number of channels before propagating the linear layers will be 3*512*512*512 = 402653184 (approx 400 million), and the number of "pixels" (flattened) will be 32x32/(2x2)/(2x2)/(2x2) = 4x4

3. If useImageDataset=True, the linear layers (numberOfLinearLayers) are initialised, propagated, and updated (if useDynamicGeneratedHiddenConnections=True) using the exact same code as useImageDataset=False (numberOfLayers).

---
numberOfSegmentsPerNeuron (Gemini 2.5 Pro prompt);

Please upgrade the entire EISANI codebase to support multiple segments per neuron (numberOfSegmentsPerNeuron):
- update EISANIpt_EISANI_globalDefs: set numberOfSegmentsPerNeuron = 8.
- update EISANIpt_EISANImodel:_initialise_layer_weights() - 
	- add a new dimension of size numberOfSegmentsPerNeuron in the dense (useSparseMatrix=False) and sparse (useSparseMatrix=True) weight matrices.
	- the dimensions of each weight matrix is now shape=[numNeurons, numberOfSegmentsPerNeuron, prevSize].
- update EISANIpt_EISANImodel:forward():_compute_layer_standard/_compute_layer_EI - when forward propagating the activations, propagate the previous layer input activations into every segment of each hidden layer neuron.
- update EISANIpt_EISANImodel:_neuronActivationFunction() - update the neuron activation function to activate a neuron if any of its segments are active.
- update EISANIpt_EISANImodel:__init__:
	- upgrade neuronSegmentAssignedMask with a new dimension (of size numberOfSegmentsPerNeuron).
	- upgrade hiddenHashes/hiddenHashesExc/hiddenHashesInh with a new dimension (of size numberOfSegmentsPerNeuron).
- update EISANIpt_EISANImodelDynamic:_dynamic_hidden_growth_vectorised()/_dynamic_hidden_growth() -
	- add argument (segmentIndexToUpdate), which defines which segment of the neurons to update. 
	- segmentIndexToUpdate will be defined based on the independent dataset being trained.
	- [the dynamic growth functions will therefore reuse already assigned neurons if their segment index (segmentIndexToUpdate) is not already assigned; otherwise a new neuron will be assigned. This requirement should not need to be explicitly coded, it should naturally emerge from constraining dynamic updates to segmentIndexToUpdate.]
- update EISANIpt_EISANImodelDynamic:perform_uniqueness_check()/perform_uniqueness_check_vectorised() - add argument (segmentIndexToUpdate), which defines which segment of a neuron to perform a uniqueness check.

---
recursiveLayers (Gemini 2.5 Pro prompt);

Please upgrade the entire EISANI codebase to support recursive layers (recursiveLayers):
- update EISANIpt_EISANI_globalDefs: 
	recursiveLayers = True
	if(recursiveLayers): 
		transformerSuperblocks = False
		if(transformerSuperblocks):
			recursiveSuperblocksNumber = 2
		else:
			recursiveSuperblocksNumber = 1
	else:
		recursiveSuperblocksNumber = 1
- update EISANIpt_EISANImodel:EISANImodel:__init__() - 
	if(recursiveLayers): 
		numberUniqueLayers = recursiveSuperblocksNumber*2
		#*2 explanation: the first forward propagated layer in a superblock always uses unique weights:
		#	- for the first superblock this will comprise unique weights between the input layer and the first hidden layer of the superblock
		#	- for the all other superblocks this will comprise unique weights between the previous superblock and the first hidden layer of the superblock
	else:
		numberUniqueLayers = numberOfHiddenLayers
	store unique weights for numberUniqueLayers instead of numberOfHiddenLayers
	store unique outputConnectionMatrix for numberUniqueLayers instead of numberOfHiddenLayers
	store neuronSegmentAssignedMask for numberUniqueLayers instead of numberOfHiddenLayers
	store hiddenHashes* for numberUniqueLayers instead of numberOfHiddenLayers
	etc
- define getUniqueLayerIndex(self, layerIdSuperblock, layerIdHidden):
	if(recursiveLayers):
		if(layerIdHidden==0):
			uniqueLayerIndex = layerIdSuperblock*2
		else:
			uniqueLayerIndex = layerIdSuperblock*2+1
	else:
		uniqueLayerIndex = layerIdHidden
	return uniqueLayerIndex
- update EISANIpt_EISANImodel:forward() - 
	update forward propagation logic;
		for layerIdSuperblock in range(recursiveSuperblocksNumber):
			for layerIdHidden in range(numberOfHiddenLayers):
				uniqueLayerIndex = self.getUniqueLayerIndex(layerIdSuperblock, layerIdHidden)
				pass uniqueLayerIndex to _compute_layer_EI/_compute_layer_standard (layerIdx argument)
				pass uniqueLayerIndex to _dynamic_hidden_growth_vectorised/_dynamic_hidden_growth (layerIdx argument)
	update output connections logic;
		if useOutputConnectionsLastLayer;
			...	#same
		else:
			actLayerIndex = 0
			for layerIdSuperblock in range(recursiveSuperblocksNumber):
				for layerIdHidden in range(numberOfHiddenLayers):
					act = layerActivations[actLayerIndex]
					uniqueLayerIndex = self.getUniqueLayerIndex(layerIdSuperblock, layerIdHidden)
					weights = self.outputConnectionMatrix[luniqueLayerIndexayerIdx]
					...	#same
					if trainOrTest and y is not None:
						self._update_output_connections(uniqueLayerIndex, act, y, device)
					actLayerIndex += 1

---
limitOutputConnectionsBasedOnPrevalence/limitOutputConnectionsBasedOnExclusivity (o3 prompt);

Please upgrade the implementation of prune_output_connections_based_on_prevalence_and_exclusivity (limitOutputConnectionsBasedOnPrevalenceOrExclusivity=True);
- prune across layers iteratively (starting with the last hidden layer):
	- when pruning output connections from the last hidden layer, use the existing code logic (limitOutputConnectionsBasedOnPrevalence and limitOutputConnectionsBasedOnExclusivity).
	- when pruning output connections from every hidden layer below the last hidden layer, use the existing code logic (limitOutputConnectionsBasedOnPrevalence and limitOutputConnectionsBasedOnExclusivity), but also verify that the hidden neuron does not have any connections remaining to a higher layer hidden neuron.
	- prune hidden neurons without any output connections (that have had all of their output connections pruned); 
		- create bool tensor hiddenNeuronsRemoved to define which hidden neurons must be removed.
		- execute new function pruneHiddenNeurons(layerIndex) to update self.hiddenConnectionMatrix*, self.hiddenNeuronSignatures* and self.neuronSegmentAssignedMask based on neurons recently removed in the layer (hiddenNeuronsRemoved).

limitOutputConnectionsBasedOnAccuracy (o3 prompt);

please now;
- assume a global bool called limitOutputConnectionsBasedOnAccuracy is defined.
- create a new pytorch tensor called hiddenNeuronPredictionAccuracy, which records an effective tuple representing the class prediction accuracy of each hidden neuron in each layer of the network. Provide a diff of the init() function with this change.
- rename the function prune_output_connections_based_on_prevalence_and_exclusivity() to prune_output_connections_and_hidden_neurons(), and add option limitOutputConnectionsBasedOnAccuracy. Provide a diff of the prune_output_connections_and_hidden_neurons() function containing this change.
- update the hiddenNeuronPredictionAccuracy tensor every time the forward function is executed. Provide a diff of the forward() function containing this change.

Please update this code to perform a softer max (instead of argmax) to determine if a hidden neuron correctly predicts the class target y;
- normalise output connection weights; weights = self.normaliseOutputConnectionWeights(self.outputConnectionMatrix)
- take a softmax across every hidden neuron's output connection weights (dim output neurons)
- a hidden neuron correctly predicts the class target y if its softmax output connection weight is above some threshold (limitOutputConnectionsSoftmaxWeightMin=0.5).

---
!useInhibition (Claude Sonnet 4 prompt);

Please upgrade EISANIpt_EISANImodel.py and EISANIpt_EISANImodelDynamic.py to support option useInhibition=False;
Do not create or assign any inhibitory neurons (if useEIneurons=True) or synapses (if useEIneurons=False).

---
useSequentialSANI:useConnectionWeights (o3 prompt);

please create a simplified version of dynamic_hidden_growth that assigns a new neuron (if unique) for every combination (pair) of activated neurons in the modified function arguments prevActivationSeg0 and prevActivationSeg1. It assigns the sparse connection to each previously activated neuron in the pair to a separate segment (segIndex0 and segIndex1).
def dynamic_hidden_growth(self, hiddenLayerIdx: int, prevActivation: torch.Tensor, device: torch.device, segmentIndexToUpdate: int) -> None : ...
def perform_uniqueness_check(self, hiddenLayerIdx, newNeuronIdx, randIdx, weights, segmentIndexToUpdate): ...
def _build_signature(self, cols: torch.Tensor, w: torch.Tensor) -> str: ...

useSequentialSANI:!useConnectionWeights (o3 prompt);

I am replacing pytorch sparse weights (COO) functions for forward propagating where a) each neuron only has a single connection (input) and b) the connections are all permanently weighted 1.0, with a more efficient and simple 1D index array. The 1D index array (indexArray) converts an index at network layer A2 to an index at previous network layer A1 (similar paradigm to a weight matrix). Please write functions for the following;
1. The forward propagation function forwardProp(self, activationsLayer1, layerIdx, pairId) converts activationsLayer1 to activationsLayer2 using either self.indexArrayA[layerIdx] or self.indexArrayB[layerIdx] (depending on whether pairId==0 or 1). Assume a bool pytorch tensor called activationsLayerA of shape [B, numNeuronSegments[layerIdx-1]]; activationsLayerA2 is of shape [B, numNeuronSegments[layerIdx]]. self.indexArrayA[layerIdx] and self.indexArrayB[layerIdx] are long tensors of shape [numNeuronSegments[layerIdx]].
2. A dynamic connection assignment function called sequentialSANI_dynamic_hidden_growth_pairwise(self, layerIdx, activationsLayerA1, activationsLayerB1). sequentialSANI_dynamic_hidden_growth_pairwise assigns new connection index pair (appends to the end of self.indexArrayA[layerIdx] and the end of self.indexArrayB[layerIdx]) for every active pair of neurons in activationsLayerA1 and activationsLayerB1. It also updates numAssignedNeuronSegments[layerIdx] based on the number of new neuron pairs assigned. sequentialSANI_dynamic_hidden_growth_pairwise should be executed in a parallel using pytorch tensor operations instead of python for loops. 
3. Before assigning the new connection index pair, sequentialSANI_dynamic_hidden_growth_pairwise executes a function called perform_uniqueness_check() to verify whether each pair has already been assigned, and if so it does not create a new pair. Implementation: it deactivates the corresponding elements in the tensor(s) used to perform the dynamic index assignment. perform_uniqueness_check should likewise be executed in a parallel using pytorch tensor operations instead of python for loops.
4. Please use static self.indexArrayA[layerIdx] and self.indexArrayB[layerIdx] arrays, providing an index number that defines how many elements they currently have (numAssignedNeuronSegments[layerId]). They always have the same number of elements (because they represent a connection pair in a binary tree network). Create another function called expandArrays(); If, the number of new pairs exceeds the connection index array capacity (ie numAssignedNeuronSegments[layerIdx] > self.indexArrayA[layerIdx].shape[0] {note same as self.indexArrayB[layerIdx].shape[0]}), grow every self.indexArrayA[layerIdX] and self.indexArrayB[layerIdX] by blockExpansionSize elements (where blockExpansionSize=1000).

---
!useSequentialSANI:limitHiddenConnections:limitHiddenConnectionsBasedOnPrevalence (o3 prompt);

Attached is code below to prune 1. network output connections and 2. unused hidden neuron segments (with pruned output connections).

I want you to create a new function called prune_hidden_neurons_and_output_connections() that 1. prunes all hidden neurons whose self.hiddenNeuronUsage is below hiddenNeuronMinUsageThreshold, along with 2. their corresponding output connections (self.outputConnectionMatrix). For step 1., you will need to;
1. modify self.neuronSegmentAssignedMask
2. modify self.hiddenConnectionMatrix* (please use prune_rows() function)
3. modify self.hiddenNeuronSignatures* (please use purge_sigs() function)

Note;
self.hiddenNeuronUsage: List[torch.Tensor] = [torch.zeros((hiddenLayerSize,), dtype=torch.float, device=device) for _ in range(self.numberUniqueHiddenLayers)]

Remember to reuse any existing functions.

---
useSequentialSANI:limitHiddenConnections:limitHiddenConnectionsBasedOnPrevalence (o3 prompt);

Please implement the following functions to a) prune hidden layer neurons (pruneHiddenNeurons) and b) prune output layer connections (prune_output_rows).

- The code should support both useSparseOutputMatrix=True and useSparseOutputMatrix=False
- The code should assume that the current layer sizes are defined by self._getCurrentLayerSize(layerIdx).

...

a)
def pruneHiddenNeurons(self, layerIndex: int, hiddenNeuronsRemoved: torch.Tensor) -> None:
	"""
	Completely delete (do not simply zero-out) hidden neurons masked by hiddenNeuronsRemoved

	Arguments
	---------
	layerIndex : int
	    Index of the unique hidden layer being pruned.
	hiddenNeuronsRemoved : torch.BoolTensor  shape = [hidden]
	    True for every neuron that must disappear.
	
	prunes these arrays;
	self.numAssignedNeuronSegments = torch.zeros(self.numberUniqueHiddenLayers, dtype=torch.long, device=device)
	self.indexArrayA: List[torch.Tensor] = [torch.full((hiddenLayerSizeStart,), -1, dtype=torch.long, device=device) for _ in range(self.numberUniqueHiddenLayers)]
	self.indexArrayB: List[torch.Tensor] = [torch.full((hiddenLayerSizeStart,), -1, dtype=torch.long, device=device) for _ in range(self.numberUniqueHiddenLayers)]

	"""
	#COMPLETE THIS FUNCTION

b)
def prune_output_rows(outputConnectionMat: torch.Tensor, hiddenNeuronsRemoved: torch.Tensor) -> torch.Tensor:
	Completely delete (do not simply zero-out) output connections whose first dimension is masked by hiddenNeuronsRemoved

	Arguments
	---------
	outputConnectionMat : torch.float	shape=[currentHiddenLayerSize, config.numberOfClasses]
	hiddenNeuronsRemoved : torch.BoolTensor  shape = [hidden]
	    True for every neuron that must disappear.
	...
	#COMPLETE THIS FUNCTION

---
useStochasticUpdates (Codex IDE GPT5 prompt);

Add option useStochasticUpdates=True in EISANIpt_EISANI_globalDefs.py, which changes the learning (i.e. network generation) algorithm to be stochastic: testing individual connection changes on output loss independently. 

Please implementation useStochasticUpdates for both case useDynamicGeneratedHiddenConnections=True and useDynamicGeneratedHiddenConnections=False;
- When useDynamicGeneratedHiddenConnections=False, useStochasticUpdates=True simply tests whether a candidate change in an existing network connection value (+1/-1) produces a positive or negative effect on the loss. If the candidate change produces a decrease in loss then the change is retained, else the change is discarded.
- When useDynamicGeneratedHiddenConnections= True, useStochasticUpdates=True tests whether a new connection (of value +1/-1) between two randomly selected network neurons (of adjacent layers) has a positive or negative effect on the loss. 
a) If the candidate connection produces a decrease in loss then the new connection is retained, else it is discarded. 
b) If i) the candidate connection already exists, ii) the candidate connection value is different than its current value (+1/-1), and iii) it produces a decrease in loss, then the change is applied (rather than creating a new connection), else the change is discarded.

When useStochasticUpdates=False, the original EISANI network generation code is used (no changes should be applied).

To assist you in this task, please integrate the LREANNpt_SUANN.py code provided below into EISANIpt_EISANI.py (trainOrTestModel, pertubation_function, perturb_once, update_weight, rand_index); 
a) As EISANIpt creates a binary network (instead of float weighted network), the stochastic learning algorithm provided below will need to be updated to simply test whether a candidate independent connection change produces a positive or negative effect on the loss (rather than taking into account the precise lossDiff of the change). 
b) As EISANIpt uses a sparse binary network rather than dense network, the code provided will need to be changed accordingly. 
c) The code provided is more suitable for useDynamicGeneratedHiddenConnections=False, as when useDynamicGeneratedHiddenConnections=True an actual new sparse connection will often need to be tested (rather than a simple weight value change).

... 
[LREANNpt_SUANN.py code]

---
useStochasticUpdates (Codex IDE GPT5 prompt);

I want you to perform a major upgrade to useStochasticUpdates that should also solve this issue;
a) When useStochasticUpdates=True, I want every hidden neuron in the network to be connected to every output neuron (use a dense matrix).
b) When useStochasticUpdates=True, the output connections are learnt by performing single layer backprop between the output neurons (classes) and every hidden neuron in the network.

Please also confirm that you understand the purpose of useStochasticUpdates and why I am performing this upgrade.

	---
	useStochasticUpdates (Codex IDE GPT5 prompt);

	upgrade useStochasticUpdates such that the input layer is initialised with (on average) equal numbers of active and inactive neurons. update continuousVarEncoding to convert input to float of size EISANICNNcontinuousVarEncodingNumBits, then convert directly to binary.

	---
	useStochasticUpdates (Codex IDE GPT5 prompt);

	upgrade useStochasticUpdates - the neuron activation function fires when input activation is above 0, else it does not fire

---
useImageDataset (GPT5 prompt);

Please add the code for case EISANICNNkernelAllPermutations=False. This new code defines 9 CNN kernel weights representing 9 unique orientations (each at a unique +45deg incremented rotation). This design specification is loosely based on the orientation pinwheel receptive fields observed in human visual cortex layer 1 (V1).

kernel #1 (0 deg);
+1 +1 +1
+1 +1 +1
-1 -1 -1

kernel #2 (45 deg);
+1 +1 +1
-1 +1 +1
-1 -1 +1

kernel #3 (90 deg);
-1 +1 +1
-1 +1 +1
-1 +1 +1

etc
etc
etc
etc
etc
etc

[EISANIpt_EISANImodelCNN.py:_init_conv_layers() code]

	---
	useImageDataset (GPT5 prompt);
	
	Can you also provide code for this modified kernel specification;

	kernel #1 (0 deg);
	+1 +1 +1
	0 0 0
	-1 -1 -1

	kernel #2 (45 deg);
	0 +1 +1
	-1 0 +1
	-1 -1 0

	kernel #3 (90 deg);
	-1 0 +1
	-1 0 +1
	-1 0 +1

---
EISANICNNarchitectureSparseRandom/DenseRandom/DensePretrained (Codex IDE GPT5Codex prompt);
	
Please upgrade EISANIpt_EISANImodelCNN.py (specifically init_conv_layers, propagate_conv_layers_sparse_random, EISANICNNarchitectureDenseRandom, EISANICNNarchitectureDensePretrained) to support additional implementation options EISANICNNarchitectureSparseRandom, EISANICNNarchitectureDenseRandom, and EISANICNNarchitectureDensePretrained. Retain the current codebase as is (do not change the EISANICNNarchitectureDivergeAllKernelPermutations and EISANICNNarchitectureDivergeLimitedKernelPermutations implementations). I have already upgraded EISANIpt_EISANI_globalDefs.py and EISANIpt_EISANImodelContinuousVarEncoding.py with the required parameters to make it easy for you to implement these architectures. Retain respect for shared numberOfConvlayers and EISANICNNmaxPoolEveryQLayers parameters (the actually number of layers of the CNN or the maxpooling methodology does not change across architectures).

1. EISANICNNarchitectureSparseRandom algorithm: 

Upgrade EISANIpt_EISANImodelCNN.py:init_conv_layers/EISANICNNarchitectureSparseRandom to generate custom sparse EISANI CNN. It needs to be implemented fully vectorised in pytorch - only the layers can be processed sequentially using python for loops.

For every binaryStateIndex in EISANICNNnumberOfRandomlySelectedInputBinaryStates:
	generate binary input from float input based on probability distribution given by float values - process each in batch (ALREADY DONE)
	For every layer in numberOfConvlayers:
		For every pixel in the layer (note that EISANICNNpaddingPolicy='same'):
			For every kernel in EISANICNNnumberKernels:
				For every pixel in the 3x3 kernel (note that CNNkernelSize=3):
					For every EISANI neuron in EISANICNNkernelSizeSANI:
						calculate its neuron activation			
		You should be able to execute summationSANIpassCNNlayer(self, prevActivation, layerIdCNN) to process every pixel in the layer in parallel:
			This executes EISANI EISANIpt_EISANImodelSummation.py:compute_layer_EI/compute_layer_standard (please ensure that the existing EISANIpt_EISANImodelSummation.py codebase supports EISANICNN model)
			I have already created class EISANIpt_EISANImodelCNN:EISANICNNmodel based on EISANIpt_EISANImodel and initialised self.EISANICNNmodel with the required sparse EISANI CNN parameters (weights).

Note EISANICNNcontinuousVarEncodingNumBits=1, however EISANICNNarchitectureSparseRandom uses EISANICNNnumberOfRandomlySelectedInputBinaryStates>1, so the binary input values are generated based on the probability distribution given by the input float values. Assume binaryStateIndices have already been produced and the batch has been expanded by a multiple of EISANICNNnumberOfRandomlySelectedInputBinaryStates - see EISANIpt_EISANImodelContinuousVarEncoding.py:sample_binary_states_from_probs() for the implementation.

2. EISANICNNarchitectureDenseRandom algorithm:

Upgrade EISANIpt_EISANImodelCNN.py:init_conv_layers/EISANICNNarchitectureDenseRandom to generate a standard CNN architecture with randomly initialised weights.

3. EISANICNNarchitectureDensePretrained algorithm:

Upgrade EISANIpt_EISANImodelCNN.cy:init_conv_layers/EISANICNNarchitectureDensePretrained to execute the CNN layers (ie non-FF layers) of a standard trained architecture for the specific dataset (eg CIFAR-10). I have provided Resnet18-breakaway/resnet18_breakaway.py code whose CNN layers can be executed in "full" (ie backprop) mode. Assume it has already been separetely pretrained to produce model parameters resnet18_full.pth (the required trained input weights).  
	